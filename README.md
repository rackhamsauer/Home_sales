# Home_sales
Home Sales Analysis with SparkSQL
The purpose of this assignment was to utilize SparkSQL with PySpark to analyze home sales data and determine key metrics. Through this project, I was able to manipulate large datasets, perform aggregations, and gain insights into home prices over various time spans.

Overview
In this project, I:

Loaded and analyzed the home sales dataset using SparkSQL with PySpark.

Created temporary views to simplify querying and analysis of the data.

Partitioned the data to improve performance and enable efficient analysis across different subsets.

Cached and uncached temporary tables for performance tuning and optimized data handling.

Calculated key metrics, including the average home price over various time spans.

The ultimate goal of the assignment was to explore the dataset and perform SQL-like operations on it using PySpark, providing valuable insights into home sales patterns.

Features
Temporary Views: Created views of the data to simplify SQL queries.
Data Partitioning: Partitioned the data based on relevant fields for better performance.
Caching: Cached temporary tables to improve query performance.
Time Span Analysis: Calculated the average home price over various time spans.
Tools and Resources Used
PySpark for running SparkSQL queries.
SparkSQL for querying and performing aggregations on the dataset.
GitHub for hosting the project.
Previous Class Materials for foundational concepts.
Peer Collaboration for solving challenges.
Tutor Assistance for additional support.
ChatGPT & Stack Overflow for troubleshooting and problem-solving.
How to Use
Clone the repository to your local machine.
Ensure that you have PySpark installed in your environment.
Load the dataset and create temporary views using SparkSQL.
Execute the provided PySpark scripts to analyze the dataset and calculate key metrics like average home price over time.
